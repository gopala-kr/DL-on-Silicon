{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nWriting tunable template and Using auto-tuner\n=============================================\n**Author**: `Lianmin Zheng <https://https://github.com/merrymercy>`_\n\nThis is an introduction tutorial to the auto-tuning module in tvm.\n\nThere are two steps in auto-tuning.\nThe first step is defining a search space.\nThe second step is running a search algorithm to explore through this space.\nIn this tutorial, you can learn how to perform these two steps in tvm.\nThe whole workflow is illustrated by a matrix multiplication example.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install dependencies\n--------------------\nTo use autotvm package in tvm, we need to install some extra dependencies.\n(change \"3\" to \"2\" if you use python2):\n\n.. code-block:: bash\n\n  pip3 install --user psutil xgboost\n\nTo make tvm run faster in tuning, it is recommended to use cython\nas FFI of tvm. In the root directory of tvm, execute\n(change \"3\" to \"2\" if you use python2):\n\n.. code-block:: bash\n\n  pip3 install --user cython\n  sudo make cython3\n\nNow return to python code. Import packages.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nimport sys\n\nimport numpy as np\nimport tvm\n\n# the module is called `autotvm`\nfrom tvm import autotvm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 1:  Define the search space\n--------------------------------\nIn this section, we will rewrite a deterministic tvm schedule code to a\ntunable schedule template. You can regard the process of search space definition\nas the parametrization of our exiting schedule code.\n\nTo begin with, here is how we implement a blocked matrix multiplication in tvm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Matmul V0: Constant tiling factor\ndef matmul_v0(N, L, M, dtype):\n    A = tvm.placeholder((N, L), name='A', dtype=dtype)\n    B = tvm.placeholder((L, M), name='B', dtype=dtype)\n\n    k = tvm.reduce_axis((0, L), name='k')\n    C = tvm.compute((N, M), lambda i, j: tvm.sum(A[i, k] * B[k, j], axis=k), name='C')\n    s = tvm.create_schedule(C.op)\n\n    # schedule\n    y, x = s[C].op.axis\n    k = s[C].op.reduce_axis[0]\n\n    yo, yi = s[C].split(y, 8)\n    xo, xi = s[C].split(x, 8)\n\n    s[C].reorder(yo, xo, k, yi, xi)\n\n    return s, [A, B, C]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parametrize the schedule\n^^^^^^^^^^^^^^^^^^^^^^^^\nIn the previous schedule code, we use a constant \"8\" as tiling factor.\nHowever, it might not be the best one because the best tiling factor depends\non real hardware environment and input shape.\n\nIf you want the schedule code to be portable across a wider range of input shapes\nand target hardware, it is better to define a set of candidate values and\npick the best one according to the measurement results on target hardware.\n\nIn autotvm, we can define a tunable parameter, or a \"knob\" for such kind of value.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Matmul V1: List candidate values\n@autotvm.template  # 1. use a decorator\ndef matmul_v1(N, L, M, dtype):\n    A = tvm.placeholder((N, L), name='A', dtype=dtype)\n    B = tvm.placeholder((L, M), name='B', dtype=dtype)\n\n    k = tvm.reduce_axis((0, L), name='k')\n    C = tvm.compute((N, M), lambda i, j: tvm.sum(A[i, k] * B[k, j], axis=k), name='C')\n    s = tvm.create_schedule(C.op)\n\n    # schedule\n    y, x = s[C].op.axis\n    k = s[C].op.reduce_axis[0]\n\n    # 2. get the config object\n    cfg = autotvm.get_config()\n\n    # 3. define search space\n    cfg.define_knob(\"tile_y\", [1, 2, 4, 8, 16])\n    cfg.define_knob(\"tile_x\", [1, 2, 4, 8, 16])\n\n    # 4. schedule according to config\n    yo, yi = s[C].split(y, cfg['tile_y'].val)\n    xo, xi = s[C].split(x, cfg['tile_x'].val)\n\n    s[C].reorder(yo, xo, k, yi, xi)\n\n    return s, [A, B, C]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we make four modifications to the previous schedule code and get\na tunable \"template\". We can explain the modifications one by one.\n\n1. Use a decorator to mark this function as a simple template\n2. Get a config object:\n   You can regard this :code:`cfg` as an argument of this function but\n   we obtain it in a different way. With this argument, this function is no longer\n   a deterministic schedule code. Instead, we can pass different configurations to\n   this function and get different schedules, so this function is a \"template\".\n\n   To make the template function more compact, we do two things in a single function.\n   (1) define a search space and (2) schedule according to an entity in this space.\n   To achieve this, we make :code:`cfg` be either\n   a :any:`ConfigSpace` or a :any:`ConfigEntity` object.\n\n   When it is a :any:`ConfigSpace`, it will collect all tunable knobs in this function and\n   build the search space.\n   When it is a :any:`ConfigEntity`, it will ignore all space definition API\n   (namely, :code:`cfg.define_XXXXX(...)`).   Instead, it stores deterministic values for\n   all tunable knobs, and we schedule according to these values.\n\n   During auto-tuning, we will first call this template with a :any:`ConfigSpace`\n   object to build the search space. Then we call this template with different :any:`ConfigEntity`\n   in the built space to get different schedules. Finally we will measure the code generated by\n   different schedules and pick the best one.\n\n3. Define two tunable knobs. The first one is :code:`tile_y` with\n   5 possible values. The second one is :code:`tile_x` with a same\n   list of possible values. These two knobs are independent, so they\n   span a search space with size = 5x5 = 25\n4. Schedule according to the deterministic values in :code:`cfg`\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use better space definition API\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIn the previous template, we manually list all possible values for a knob.\nThis is the lowest level API to define the space.\nHowever, we also provide another set of API to make the space definition\neasier and smarter. It is recommended to use this set of high level API.\n\nIn the flowing example, we use :any:`ConfigSpace.define_split` to define a split\nknob. It will enumerate all the possible ways to split an axis and construct\nthe space.\n\nWe also have :any:`ConfigSpace.define_reorder` for reorder knob and\n:any:`ConfigSpace.define_annotate` for annotation like unroll, vectorization,\nthread binding.\nWhen the high level API cannot meet your requirement, you can always fall\nback to use low level API.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@autotvm.template\ndef matmul(N, L, M, dtype):\n    A = tvm.placeholder((N, L), name='A', dtype=dtype)\n    B = tvm.placeholder((L, M), name='B', dtype=dtype)\n\n    k = tvm.reduce_axis((0, L), name='k')\n    C = tvm.compute((N, M), lambda i, j: tvm.sum(A[i, k] * B[k, j], axis=k), name='C')\n    s = tvm.create_schedule(C.op)\n\n    # schedule\n    y, x = s[C].op.axis\n    k = s[C].op.reduce_axis[0]\n\n    ##### define space begin #####\n    cfg = autotvm.get_config()\n    cfg.define_split(\"tile_y\", y, num_outputs=2)\n    cfg.define_split(\"tile_x\", x, num_outputs=2)\n    ##### define space end #####\n\n    # schedule according to config\n    yo, yi = cfg[\"tile_y\"].apply(s, C, y)\n    xo, xi = cfg[\"tile_x\"].apply(s, C, x)\n\n    s[C].reorder(yo, xo, k, yi, xi)\n\n    return s, [A, B, C]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>More Explanation on :code:`cfg.defile_split`</p></div>\n\n In this template, :code:`cfg.define_split(\"tile_y\", y, num_outputs=2)` will enumerate\n all possible combinations that can split axis y into two axes with factors of the length of y.\n For example, if the length of y is 32 and we want to split it into two axes\n using factors of 32, then there are 6 possible values for\n (length of outer axis, length of inner axis) pair, namely\n (32, 1), (16, 2), (8, 4), (4, 8), (2, 16) or (1, 32).\n They are just the 6 possible values of `tile_y`.\n\n During schedule, :code:`cfg[\"tile_y\"]` is a :code:`SplitEntity` object.\n We stores the lengths of outer axes and inner axes in :code:`cfg['tile_y'].size`\n (a tuple with two elements).\n In this template, we apply it by using :code:`yo, yi = cfg['tile_y'].apply(s, C, y)`.\n Actually, this is equivalent to\n :code:`yo, yi = s[C].split(y, cfg[\"tile_y\"].size[1])`\n or  :code:`yo, yi = s[C].split(y, nparts=cfg['tile_y\"].size[0])`\n\n The advantage of using cfg.apply API is that it makes multi-level split\n (when num_outputs >= 3) easier.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 2:  Search through the space\n---------------------------------\nIn step 1, we build the search space by extending our old schedule code\ninto a template. The next step is to pick a tuner and explore in this space.\n\nAuto-tuners in tvm\n^^^^^^^^^^^^^^^^^^\nThe job for a tuner can be described by following pseudo code\n\n  .. code-block:: c\n\n   ct = 0\n   while ct < max_number_of_trials:\n       propose a batch of configs\n       measure this batch of configs on real hardware and get results\n       ct += batch_size\n\nWhen proposing the next batch of configs, the tuner can take different strategies. We\nprovide four tuners with different strategies in autotvm.\n\n* :any:`RandomTuner`: Enumerate the space in a random order\n* :any:`GridSearchTuner`: Enumerate the space in a grid search order\n* :any:`GATuner`: Using genetic algorithm to search through the space\n* :any:`XGBTuner`: Uses a model based method. Train a XGBoost model to predict the speed of lowered IR and pick the next batch according to the prediction.\n\nYou can choose the tuner according to the size of your space, your time budget and other factors.\nFor example, if your space is very small (less than 1000), a gridsearch tuner or a\nrandom tuner is good enough. If your space is at the level of 10^9 (this is the space\nsize of a conv2d operator on CUDA GPU), XGBoostTuner can explore more efficiently\nand find better configs.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Begin tuning\n^^^^^^^^^^^^\nHere we continue our matrix multiplication example.\nFirst we should create a tuning task.\nWe can also inspect the initialized search space.\nIn this case, for a 512x512 square matrix multiplication, the space size\nis 10x10=100\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N, L, M = 512, 512, 512\ntask = autotvm.task.create(matmul, args=(N, L, M, 'float32'), target='llvm')\nprint(task.config_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we need to define how to measure the generated code and pick a tuner.\nSince our space is small, a random tuner is just okay.\n\nWe only make 10 trials in this tutorial for demonstration. In practice,\nyou can do more trials according to your time budget.\nWe will log the tuning results into a log file. This file can be\nused to get the best config later.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# logging config (for printing tuning log to the screen)\nlogging.getLogger('autotvm').setLevel(logging.DEBUG)\nlogging.getLogger('autotvm').addHandler(logging.StreamHandler(sys.stdout))\n\n# There are two steps for measuring a config: build and run.\n# By default, we use all cpu cores to compile program. Then measure them sequentially.\n# We measure 5 times and take average to reduce variance.\nmeasure_option = autotvm.measure_option(\n    builder='local',\n    runner=autotvm.LocalRunner(number=5))\n\n# begin tuning, log records to file `matmul.log`\ntuner = autotvm.tuner.RandomTuner(task)\ntuner.tune(n_trial=10,\n           measure_option=measure_option,\n           callbacks=[autotvm.callback.log_to_file('matmul.log')])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally we apply history best from the cache file and check its correctness.\nWe can call the function :code:`matmul` directly under the \n:any:`autotvm.apply_history_best` context. When we call this function,\nit will query the dispatch context with its argument and get the best config \nwith the same argument.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# apply history best from log file\nwith autotvm.apply_history_best('matmul.log'):\n    with tvm.target.create(\"llvm\"):\n        s, arg_bufs = matmul(N, L, M, 'float32')\n        func = tvm.build(s, arg_bufs)\n\n# check correctness\na_np = np.random.uniform(size=(N, L)).astype(np.float32)\nb_np = np.random.uniform(size=(L, M)).astype(np.float32)\nc_np = a_np.dot(b_np)\n\nc_tvm = tvm.nd.empty(c_np.shape)\nfunc(tvm.nd.array(a_np), tvm.nd.array(b_np), c_tvm)\n\nnp.testing.assert_allclose(c_np, c_tvm.asnumpy(), rtol=1e-2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
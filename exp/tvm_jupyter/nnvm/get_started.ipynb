{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nGet Started with NNVM\n=====================\n**Author**: `Tianqi Chen <https://tqchen.github.io/>`_\n\nThis article is an introductory tutorial to workflow in NNVM.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import nnvm.compiler\nimport nnvm.symbol as sym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Declare Computation\n-------------------\nWe start by describing our need using computational graph.\nMost deep learning frameworks use computation graph to describe\ntheir computation. In this example, we directly use\nNNVM's API to construct the computational graph.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>In a typical deep learning compilation workflow,\n  we can get the models from :any:`nnvm.frontend`</p></div>\n\nThe following code snippet describes $z = x + \\sqrt{y}$\nand creates a nnvm graph from the description.\nWe can print out the graph ir to check the graph content.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = sym.Variable(\"x\")\ny = sym.Variable(\"y\")\nz = sym.elemwise_add(x, sym.sqrt(y))\ncompute_graph = nnvm.graph.create(z)\nprint(\"-------compute graph-------\")\nprint(compute_graph.ir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compile\n-------\nWe can call :any:`nnvm.compiler.build` to compile the graph.\nThe build function takes a shape parameter which specifies the\ninput shape requirement. Here we only need to pass in shape of ``x``\nand the other one will be inferred automatically by NNVM.\n\nThe function returns three values. ``deploy_graph`` contains\nthe final compiled graph structure. ``lib`` is a :any:`tvm.module.Module`\nthat contains compiled CUDA functions. We do not need the ``params``\nin this case.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "shape = (4,)\ndeploy_graph, lib, params = nnvm.compiler.build(\n    compute_graph, target=\"cuda\", shape={\"x\": shape}, dtype=\"float32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can print out the IR of ``deploy_graph`` to understand what just\nhappened under the hood. We can find that ``deploy_graph`` only\ncontains a single operator ``tvm_op``. This is because NNVM\nautomatically fused the operator together into one operator.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"-------deploy graph-------\")\nprint(deploy_graph.ir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us also peek into content of ``lib``.\nTypically a compiled TVM CUDA module contains a host module(lib)\nand a device module(``lib.imported_modules[0]``) that contains the CUDA code.\nWe print out the the generated device code here.\nThis is exactly a fused CUDA version of kernel that the graph points to.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"-------deploy library-------\")\nprint(lib.imported_modules[0].get_source())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deploy and Run\n--------------\nNow that we have have compiled module, let us run it.\nWe can use :any:`graph_runtime <tvm.contrib.graph_runtime.create>`\nin tvm to create a deployable :any:`GraphModule <tvm.contrib.graph_runtime.GraphModule>`.\nWe can use the :any:`set_input <tvm.contrib.graph_runtime.GraphModule.set_input>`,\n:any:`run <tvm.contrib.graph_runtime.GraphModule.run>` and\n:any:`get_output <tvm.contrib.graph_runtime.GraphModule.get_output>` function\nto set the input, execute the graph and get the output we need.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tvm\nimport numpy as np\nfrom tvm.contrib import graph_runtime, util\n\nmodule = graph_runtime.create(deploy_graph, lib, tvm.gpu(0))\nx_np = np.array([1, 2, 3, 4]).astype(\"float32\")\ny_np = np.array([4, 4, 4, 4]).astype(\"float32\")\n# set input to the graph module\nmodule.set_input(x=x_np, y=y_np)\n# run forward computation\nmodule.run()\n# get the first output\nout = module.get_output(0, out=tvm.nd.empty(shape))\nprint(out.asnumpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Provide Model Parameters\n------------------------\nMost deep learning models contains two types of inputs: parameters\nthat remains fixed during inference and data input that need to\nchange for each inference task. It is helpful to provide these\ninformation to NNVM. Let us assume that ``y`` is the parameter\nin our example. We can provide the model parameter information\nby the params argument to :any:`nnvm.compiler.build`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "deploy_graph, lib, params = nnvm.compiler.build(\n    compute_graph, target=\"cuda\", shape={\"x\": shape}, params={\"y\": y_np})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This time we will need params value returned by :any:`nnvm.compiler.build`.\nNNVM applys  optimization  to pre-compute the intermediate values in\nthe graph that can be determined by parameters. In this case\n$\\sqrt{y}$ can be pre-computed. The pre-computed values\nare returned as new params. We can print out the new compiled library\nto confirm that the fused kernel only now contains add.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"-----optimized params-----\")\nprint(params)\nprint(\"-------deploy library-------\")\nprint(lib.imported_modules[0].get_source())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the Deployed Module\n------------------------\nWe can save the ``deploy_graph``, ``lib`` and ``params`` separately\nand load them back later. We can use :any:`tvm.module.Module` to export\nthe compiled library. ``deploy_graph`` is saved in json format and ``params``\nis serialized into a bytearray.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "temp = util.tempdir()\npath_lib = temp.relpath(\"deploy.so\")\nlib.export_library(path_lib)\nwith open(temp.relpath(\"deploy.json\"), \"w\") as fo:\n    fo.write(deploy_graph.json())\nwith open(temp.relpath(\"deploy.params\"), \"wb\") as fo:\n    fo.write(nnvm.compiler.save_param_dict(params))\nprint(temp.listdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can load the module back.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loaded_lib = tvm.module.load(path_lib)\nloaded_json = open(temp.relpath(\"deploy.json\")).read()\nloaded_params = bytearray(open(temp.relpath(\"deploy.params\"), \"rb\").read())\nmodule = graph_runtime.create(loaded_json, loaded_lib, tvm.gpu(0))\nparams = nnvm.compiler.load_param_dict(loaded_params)\n# directly load from byte array\nmodule.load_params(loaded_params)\nmodule.run(x=x_np)\n# get the first output\nout = module.get_output(0, out=tvm.nd.empty(shape))\nprint(out.asnumpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deploy using Another Language\n-----------------------------\nWe use python in this example for demonstration.\nWe can also deploy the compiled modules with other languages\nsupported by TVM such as  c++, java, javascript.\nThe graph module itself is fully embedded in TVM runtime.\n\nThe following block demonstrates how we can directly use TVM's\nruntime API to execute the compiled module.\nYou can find similar runtime API in TVMRuntime of other languages.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fcreate = tvm.get_global_func(\"tvm.graph_runtime.create\")\nctx = tvm.gpu(0)\ngmodule = fcreate(loaded_json, loaded_lib, ctx.device_type, ctx.device_id)\nset_input, get_output, run = gmodule[\"set_input\"], gmodule[\"get_output\"], gmodule[\"run\"]\nset_input(\"x\", tvm.nd.array(x_np))\ngmodule[\"load_params\"](loaded_params)\nrun()\nout = tvm.nd.empty(shape)\nget_output(0, out)\nprint(out.asnumpy())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}